
[[ch-6]]
= Features of Computer Vision Workloads on Azure


== Learning Objectives

By the end of this chapter, you will be able to:

[arabic]
. Identify the core Azure services used for computer vision and explain when to use each one
. Describe how digital images are represented as pixel data and how computers process that data
. Explain the purpose of image filters and how convolutional filtering transforms visual information
. Differentiate between image classification, object detection, optical character recognition, and facial detection
. Describe the architecture and function of convolutional neural networks (CNNs)
. Explain how multimodal models extend traditional computer vision by combining visual and textual understanding
. Discuss responsible AI considerations specific to computer vision, including fairness, privacy, and transparency


[[sec-6-1]]
== Overview of Computer Vision

Computer vision is a branch of artificial intelligence that enables machines to interpret and act on visual information such as photographs, video frames, and scanned documents. Rather than simply storing an image as a file, computer vision systems analyze the content of that image — identifying objects, reading text, recognizing faces, or generating descriptive captions.

This domain accounts for approximately *15–20%* of the AI-900 exam content, making it one of the five major pillars you need to understand. The questions in this domain test your ability to identify the right Azure service for a given scenario, understand how images are processed at the pixel level, and recognize the ethical considerations that accompany visual AI.

[.definition]
.Definition: Computer Vision
****
A field of artificial intelligence focused on enabling computers to derive meaningful information from visual inputs such as images and videos. Applications range from automated quality inspection in manufacturing to assistive technology for people with visual impairments.
****



[[sec-6-2]]
== Azure Computer Vision Services

Microsoft Azure offers several services for computer vision tasks. Each service addresses different needs, and understanding which service fits a given scenario is essential for the AI-900 exam.

=== Service Comparison

.Figure 6.5: Azure Computer Vision Services Map
image::ch6/visual_6_5_cv_services_map.png[Service ecosystem diagram showing Azure Computer Vision branches to AI Vision; Custom Vision; AI Face; and Video Indexer,width=85%]


[width="100%",cols="34%,33%,33%",options="header",]
|===
|Service |Purpose |Best Used When
|*Azure AI Vision* |General-purpose image and video analysis (captioning, tagging, object detection, OCR) |You need a dedicated computer vision toolkit for analyzing visual content
|*Azure AI Services* |A unified endpoint for multiple AI capabilities, including vision, language, and speech |Your application requires several AI features beyond vision alone, and you want a single access key
|*Azure AI Custom Vision* |Build and train custom image classification and object detection models using your own labeled data |Off-the-shelf models do not recognize the specific objects or categories your project requires
|*Azure AI Face* |Detect, analyze, and recognize human faces in images |Your application involves identity verification, facial detection, or face-based access control
|*Azure AI Video Indexer* |Extract insights from video content using more than 30 AI models |You need to analyze video for transcription, object tracking, content moderation, or scene detection
|===

=== Azure AI Vision

Azure AI Vision is the primary service for general computer vision tasks. It provides pre-built models that can analyze images and return structured results without requiring you to train your own model. Capabilities include generating image captions, applying descriptive tags, detecting objects with bounding boxes, and extracting text through OCR.

Results from Azure AI Vision are returned as JSON, which includes the analysis output along with metadata such as image dimensions and a confidence score. The confidence score is a decimal value between 0 and 1 that indicates how certain the model is about its prediction.

=== Azure AI Custom Vision

When pre-built models do not meet your needs, Azure AI Custom Vision allows you to create tailored image recognition models. You upload your own images, label them with tags that represent your categories, and train a model that learns to recognize those specific categories. For example, an agricultural technology company might train a model to distinguish between healthy crops and crops affected by disease.

Custom Vision supports two project types:

* *Classification:* Assigns one or more labels to an entire image (e.g., "`healthy`" or "`diseased`")
* *Object detection:* Identifies and locates specific objects within an image using bounding boxes

You can interact with Custom Vision through its web portal, REST API, or client SDKs.

=== Azure AI Face

The Azure AI Face service provides algorithms for detecting, recognizing, and analyzing human faces. It can locate faces within an image, identify facial landmarks (such as the positions of the eyes, nose, and mouth), and match detected faces against a known set of identities.

Due to the sensitive nature of facial recognition technology, Microsoft restricts access to certain features of this service. Only approved customers and partners who meet specific eligibility criteria and responsible use requirements can use identity recognition capabilities. This restriction reflects Microsoft's commitment to responsible AI practices.

=== Azure AI Video Indexer

Video Indexer goes beyond still images by analyzing video content. It leverages a suite of AI models to extract information such as spoken words (transcription), on-screen text (OCR), visible objects, faces, scene changes, and even emotional tone in audio. This service is useful for media companies, educational institutions, and any organization that manages large volumes of video content.

[.reflection]
.Reflection
****
Consider an industry you are familiar with — retail, healthcare, education, or manufacturing. Which of the Azure computer vision services described above would be most useful in that industry, and what specific problem would it solve?
****



[[sec-6-3]]
== Computer Vision Capabilities

Azure AI Vision provides several distinct capabilities, each suited to different analytical tasks. The table below summarizes these capabilities and their common applications.

[width="100%",cols="34%,33%,33%",options="header",]
|===
|Capability |What It Does |Example Use Cases
|*Image Captioning* |Generates a natural-language description of an image along with a confidence score (0 to 1) |Automatic alt-text generation for web accessibility; photo library description
|*Tagging* |Assigns descriptive keyword labels to an image, each with a confidence score |Organizing digital asset libraries; enabling keyword search across image collections
|*Object Detection* |Identifies objects within an image and provides bounding box coordinates for each |Inventory tracking on retail shelves; vehicle counting in traffic analysis
|*Facial Detection* |Locates human faces in an image without identifying who the faces belong to (see <<sec-6-9,Section 6.9>> for the distinction between detection and recognition) |Crowd counting at events; blurring faces for privacy in public footage
|*Face Recognition* |Provided by *Azure AI Face* (not Azure AI Vision); matches detected faces against enrolled identities to determine who a person is (see <<sec-6-9,Section 6.9>> for the distinction between detection and recognition). _Note: Azure AI Vision provides facial detection only; face recognition is a capability of the Azure AI Face service._ |Secure building access; identity verification during onboarding
|*OCR* |Extracts printed or handwritten text from images and converts it to machine-readable characters |Digitizing paper records; scanning receipts; reading text from street signs
|===

[.definition]
.Definition: Confidence Score
****
A numerical value, typically between 0 and 1, that represents how certain an AI model is about a particular prediction. A score of 0.95 indicates high confidence, while a score of 0.40 indicates substantial uncertainty.
****


[TIP]
====
For the AI-900 exam, be prepared to match each capability to the correct scenario. Ask yourself: "`Does this scenario involve identifying _what_ is in the image (classification), _where_ something is (object detection), _what text_ is present (OCR), or _whose face_ is shown (face recognition)?`"
====



[[sec-6-4]]
== How Computer Vision Works: Pixels and Image Representation

.Figure 6.3: Pixel Grid and RGB Channel Decomposition
image::ch6/visual_6_3_pixel_rgb.png[Image decomposed into pixel grid with separate red; green; and blue channel views,width=85%]


To understand computer vision, you need to understand how computers represent images.

=== Pixels as the Building Blocks

A digital image is a grid of tiny squares called *pixels* (short for "`picture elements`"). Each pixel stores color information as numerical values. The total number of pixels in an image defines its *resolution*. For example, an image that is 1920 pixels wide and 1080 pixels tall contains over two million pixels.

[.definition]
.Definition: Pixel
****
The smallest unit of a digital image, represented by numerical values that encode brightness and color information. Each pixel corresponds to a single point in the image grid.
****


=== Grayscale Images

In a grayscale image, each pixel holds a single numerical value between 0 and 255. A value of 0 represents pure black, 255 represents pure white, and intermediate values represent shades of gray. The entire image can be thought of as a two-dimensional array of numbers — rows and columns of intensity values.

=== Color Images and RGB Channels

Color images use three separate layers, called *channels*, to represent color. Each channel corresponds to one of the three primary colors of light:

* *Red channel* — intensity of red (0 to 255)
* *Green channel* — intensity of green (0 to 255)
* *Blue channel* — intensity of blue (0 to 255)

Each pixel in a color image is therefore described by three values. For example, the RGB values (255, 0, 0) produce pure red, (0, 255, 0) produce pure green, and (128, 0, 128) produce purple. By blending these three channels together, the full spectrum of visible color can be represented.

Think of RGB values like mixing light. When red and blue light are mixed in equal amounts with no green, they produce purple. Your screen works this way with every pixel — combining different intensities of red, green, and blue light to create the full spectrum of visible colors.

[.definition]
.Definition: RGB Color Model
****
A color model in which red, green, and blue light are combined in varying intensities (each ranging from 0 to 255) to produce a wide array of colors. Most digital displays and computer vision systems use RGB as their primary color representation.
****


=== From Pixels to Patterns

Computer vision algorithms do not "`see`" an image the way humans do. Instead, they analyze the numerical values in the pixel grid to identify mathematical patterns — gradients where values change sharply (edges), regions of similar values (textures), and spatial relationships between features. These patterns become the raw material that machine learning models use to make predictions about what an image contains.


[[sec-6-5]]
== Image Filters and Convolutional Filtering

=== What Is a Filter?

An image filter is a mathematical operation that transforms an image by modifying pixel values in a systematic way. Filters are defined by small grids of numbers called *kernels* (also known as filter matrices). The kernel is applied to the image by sliding it across every position in the pixel grid, performing a calculation at each location, and writing the result to a new output image.

This process of sliding a kernel across an image is called *convolution*, and it forms the foundation of both traditional image processing and modern deep learning for vision.

[.definition]
.Definition: Convolution
****
A mathematical operation that slides a kernel across an image, computing weighted sums of pixel neighborhoods to produce a transformed output. Convolution is the core operation in both traditional image filtering and convolutional neural networks.
****


=== How Convolution Works

.Figure 6.2: Convolution Operation Step-by-Step
image::ch6/visual_6_2_convolution_operation.png[Multi-panel illustration showing kernel sliding over pixel grid with element-wise multiplication,width=85%]


The convolution operation follows these steps:

[arabic]
. Place the kernel over a section of the image (e.g., a 3x3 block of pixels)
. Multiply each pixel value by the corresponding value in the kernel
. Sum all the products to produce a single output value
. Move the kernel one pixel to the right (or down) and repeat

The result is a new image — sometimes called a *feature map* — where each pixel reflects the combined influence of its neighbors as weighted by the kernel.

[.definition]
.Definition: Feature Map
****
The output produced by applying a convolutional filter to an image, highlighting specific patterns such as edges or textures. Each feature map represents the presence and location of a particular visual pattern detected by the kernel.
****


=== Common Filter Types

.Figure 6.7: Common Image Filter Types
image::ch6/visual_6_7_filter_types.png[Before/after panels showing blurring; sharpening; edge detection; and color inversion filters,width=85%]


[width="100%",cols="34%,33%,33%",options="header",]
|===
|Filter Type |Effect |Typical Use
|*Blurring* |Averages neighboring pixel values, softening details and reducing noise |Preprocessing images before analysis; reducing visual noise
|*Sharpening* |Enhances differences between adjacent pixels, making edges more distinct |Highlighting fine details; improving clarity of features
|*Edge Detection* |Identifies boundaries where pixel intensity changes rapidly |Isolating shapes and outlines; detecting objects in scenes
|*Color Inversion* |Subtracts each pixel value from the maximum intensity, producing a negative |Creating visual effects; enhancing contrast for certain analyses
|===

[.definition]
.Definition: Kernel (Filter Matrix)
****
A small grid of numerical values used in convolution. The kernel determines what effect the filter has on the image. Different kernels produce different transformations, such as blurring, sharpening, or edge detection.
****


[TIP]
====
You do not need to memorize kernel values for the AI-900 exam. Focus on understanding the _concept_ — that filters transform images by applying mathematical operations to pixel neighborhoods, and that different kernels produce different visual effects.
====


*Real-World Connection:* Edge detection filters are used in self-driving car systems to identify road boundaries, lane markings, and obstacles. Sharpening filters help medical imaging systems enhance the detail in X-rays and MRI scans for more accurate diagnosis.

[.reflection]
.Reflection
****
Before moving on, can you describe in your own words what a kernel does and what a feature map represents? Why do you think different kernels produce different visual effects when applied to the same image?
****



[[sec-6-6]]
== Image Classification

*Image classification* is the task of assigning a label or category to an entire image based on its visual content. For example, a classification model might determine whether a photograph shows a cat, a dog, or a bird.

=== How Classification Works

Image classification typically relies on *supervised learning* (a training approach in which the model learns from labeled examples — introduced in xref:chapter-4.adoc#ch-4[Chapter 4]: Fundamental Principles of Machine Learning). During training, the model is presented with a large dataset of images, each paired with its correct label. The model learns to associate visual patterns — shapes, colors, textures — with specific categories. After training, the model can predict the label for new images it has never seen before.

Unsupervised learning, which works with unlabeled data, is covered in xref:chapter-4.adoc#ch-4[Chapter 4].

=== Practical Applications

* *Healthcare:* Analyzing medical images (X-rays, MRIs) to detect abnormalities such as tumors or fractures
* *Agriculture:* Identifying crop diseases from aerial or field photographs
* *Manufacturing:* Inspecting products on an assembly line for defects
* *Wildlife conservation:* Classifying animal species from camera trap images

=== Classification in Azure

Azure AI Custom Vision is the primary service for building custom image classification models. You upload labeled training images, train the model, and then deploy it to classify new images via a REST API. Azure AI Vision also provides pre-built classification capabilities that work without custom training for common scenarios.

[NOTE]
====
Think of a situation in your daily life or work where automatically sorting images into categories would save time or reduce errors. What categories would your classifier need, and what kind of training images would you need to collect?
====



[[sec-6-7]]
== Object Detection

While image classification answers the question "`What is in this image?`", *object detection* goes further by also answering "`Where is it?`"

=== Bounding Boxes

.Figure 6.8: Bounding Box Anatomy
image::ch6/visual_6_8_bounding_box.png[Annotated image showing bounding boxes with object labels; confidence scores; and pixel coordinates,width=85%]


Object detection models identify objects within an image and draw *bounding boxes* around each one. A bounding box is a rectangle defined by pixel coordinates that frames the detected object.

[.definition]
.Definition: Bounding Box
****
A rectangular outline defined by pixel coordinates that frames a detected object within an image. Bounding boxes indicate both the location and spatial extent of each object identified by a detection model.
****


Along with each bounding box, the model returns:

* The *label* (what the object is)
* A *confidence score* (how certain the model is)
* The *coordinates* of the box (where the object is located)

=== How Object Detection Differs from Classification

.Figure 6.4: Computer Vision Task Comparison
image::ch6/visual_6_4_cv_task_comparison.png[Same image shown four ways: classification; object detection with bounding boxes; semantic segmentation; and OCR,width=85%]


[width="100%",cols="34%,33%,33%",options="header",]
|===
|Aspect |Image Classification |Object Detection
|*Question answered* |"`What is in the image?`" |"`What objects are in the image and where are they?`"
|*Output* |A single label (or set of labels) for the entire image |Multiple labels, each with bounding box coordinates
|*Spatial awareness* |No — treats the image as a whole |Yes — pinpoints the location of each object
|*Training complexity* |Requires labeled images |Requires labeled images _plus_ bounding box annotations
|===

=== Use Cases

* *Retail:* Monitoring shelf inventory by detecting products and their positions
* *Transportation:* Counting vehicles and detecting traffic violations
* *Security:* Identifying unauthorized objects in restricted areas
* *Autonomous systems:* Detecting pedestrians, vehicles, and obstacles for self-driving vehicles

Object detection in Azure is available through both Azure AI Vision (pre-built models) and Azure AI Custom Vision (custom-trained models).


[[sec-6-8]]
== Optical Character Recognition (OCR)

*Optical Character Recognition (OCR)* is the process of extracting text from images and converting it into machine-readable characters. This enables computers to "`read`" printed or handwritten text that appears in photographs, scanned documents, signs, labels, and other visual media.

=== Traditional OCR vs. AI-Powered OCR

Traditional OCR systems relied on pattern matching — comparing the shapes of characters in an image against a library of known character templates. Modern AI-powered OCR uses machine learning models that analyze the shapes, strokes, and context of characters. This approach handles challenging scenarios far better, including:

* Unusual or decorative fonts
* Handwritten text with variable letter shapes
* Skewed, rotated, or partially obscured text
* Low-quality or noisy images

=== OCR in Azure

Azure provides OCR capabilities through its *Read API*, which is part of Azure AI Vision. The Read API supports:

* *Synchronous processing* for single images (fast, real-time results)
* *Asynchronous processing* for large, multi-page documents (designed for high-volume batch operations)
* Multiple languages and both printed and handwritten text

Azure also offers *Document Intelligence* (formerly Form Recognizer), which builds on OCR technology to go beyond raw text extraction. Document Intelligence understands the _structure_ of documents — it can identify fields, tables, key-value pairs, and specific document types such as invoices, receipts, and identity documents.

=== OCR Output Structure

.Figure 6.10: OCR Output Structure
image::ch6/visual_6_10_ocr_output.png[Document image with overlaid bounding polygons and adjacent simplified JSON output,width=85%]


When you submit an image to Azure's Read API, the response includes:

* *Lines of text* detected in the image
* *Individual words* within each line
* *Bounding polygon coordinates* showing where each line and word appears in the image
* *Confidence scores* for each detected word

[.definition]
.Definition: OCR (Optical Character Recognition)
****
A technology that identifies and extracts text from images, converting visual representations of characters into machine-readable digital text. Modern OCR systems use machine learning to handle diverse fonts, handwriting, and challenging image conditions.
****


[.definition]
.Definition: Document Intelligence
****
An Azure AI service that extends OCR by understanding document structure. It can extract fields, tables, and key-value pairs from forms, invoices, receipts, and other structured documents.
****



[[sec-6-9]]
== Facial Detection and Analysis

Facial detection and facial analysis are related but distinct capabilities within computer vision.

=== Facial Detection vs. Face Recognition

.Figure 6.6: Facial Detection vs. Analysis vs. Recognition
image::ch6/visual_6_6_face_detection_analysis_recognition.png[Three-stage progression from facial detection to analysis to identity recognition,width=85%]


The following table compares facial detection, facial analysis, and face recognition to clarify what each capability does and whether it can identify specific individuals:

[width="100%",cols="34%,33%,33%",options="header",]
|===
|Capability |What It Does |Identifies Individuals?
|*Facial Detection* |Locates faces within an image and may return the position and size of each face |No
|*Facial Analysis* |Examines detected faces to extract attributes such as estimated age, head pose, or the presence of accessories (e.g., glasses) |No
|*Face Recognition* |Matches a detected face against a database of known identities to determine who the person is |Yes
|===

Facial detection is the foundational step. It finds faces in an image and returns bounding box coordinates. Facial analysis adds attribute information. Face recognition goes the furthest by linking a detected face to a specific identity.

=== Underlying Technology

These capabilities rely on deep learning models, particularly convolutional neural networks (covered in detail in <<sec-6-10,Section 6.10>>), trained on large and diverse datasets of human faces. The models learn to identify facial landmarks — specific points on the face such as the corners of the eyes, the tip of the nose, and the edges of the mouth. These landmarks are returned as coordinate data in the JSON response.

=== Applications

* *Security and access control:* Unlocking devices or granting building access based on facial verification
* *Retail analytics:* Counting foot traffic or estimating demographic patterns (where permitted and ethical)
* *Healthcare:* Exploring facial analysis as a potential tool for assessing certain conditions
* *Media and entertainment:* Automatically tagging people in photo collections or applying visual effects in video

=== Access Restrictions

Because facial recognition technology carries significant ethical and privacy risks, Microsoft requires customers to apply for access to the identification and verification features of the Azure AI Face service. Approval is granted based on documented use cases that align with Microsoft's responsible AI standards.

[TIP]
====
Remember the distinction between facial _detection_ (finding a face) and face _recognition_ (identifying whose face it is). The AI-900 exam may present scenarios where you need to choose the correct term.
====



[[sec-6-10]]
== Convolutional Neural Networks (CNNs)

Convolutional neural networks are the deep learning architecture most closely associated with computer vision. CNNs are specifically designed to process grid-structured data such as images.

=== CNN Architecture

.Figure 6.1: CNN Architecture Pipeline
image::ch6/visual_6_1_cnn_architecture.png[Left-to-right pipeline from input image through convolutional; pooling; and fully connected layers to output,width=85%]


A CNN consists of several types of layers, each performing a specific role in the analysis pipeline. The following table describes each layer type and its function:

[width="100%",cols="50%,50%",options="header",]
|===
|Layer Type |Function
|*Convolutional Layers* |Apply filters (kernels) to the input image to produce feature maps that capture patterns such as edges, textures, and shapes
|*Pooling Layers* |Reduce the spatial dimensions of feature maps (typically using max pooling) to retain the most important information while decreasing computational load
|*Fully Connected Layers* |Combine all extracted features into a single vector and use it to make a final prediction (e.g., assigning a class label)
|*Activation Functions* |Introduce non-linearity into the network; in the output layer, functions like softmax convert raw scores into probability distributions across classes
|===

Data flows through these layers sequentially: an image enters as a pixel grid, convolutional layers produce feature maps highlighting detected patterns, pooling layers compress those maps to reduce complexity, fully connected layers combine all extracted features, an activation function produces probability scores, and the highest score becomes the predicted label.

=== How a CNN Learns

[arabic]
. *Initial state:* The network begins with random filter weights
. *Forward pass:* An image passes through the layers, and the network produces a prediction
. *Loss calculation:* The prediction is compared to the correct label, and the error (loss) is computed
. *Backpropagation:* The error is propagated backward through the network, and each filter weight is adjusted to reduce the error (see xref:chapter-4.adoc#ch-4[Chapter 4], xref:chapter-4.adoc#sec-4-7[Section 4.7] for neural network fundamentals)
. *Iteration:* This process repeats over thousands or millions of training images, and the network's accuracy improves with each cycle

[TIP]
====
Without looking back, can you name the four types of layers in a CNN and describe what each one does? Try it, then verify your answers in the table above.
====


[.definition]
.Definition: Convolutional Neural Network (CNN)
****
A type of deep learning model designed for processing grid-structured data, especially images. CNNs use convolutional layers to automatically learn spatial features, pooling layers to reduce dimensionality, and fully connected layers to make predictions.
****


[.definition]
.Definition: Backpropagation
****
The process by which a neural network adjusts its internal weights after making an incorrect prediction. The error is calculated at the output and propagated backward through the network layers, allowing each weight to be updated to improve future predictions.
****


=== Beyond Classification

While CNNs are most commonly associated with image classification, they serve as the backbone for many other vision tasks:

* *Object detection:* CNNs identify and locate multiple objects within a single image
* *Image segmentation:* CNNs classify every individual pixel in an image, creating detailed maps of distinct regions (used extensively in medical imaging)
* *Video analysis:* CNNs can be applied to individual video frames to detect and track objects over time

*Real-World Connection:* The Azure AI Vision service you explored in <<sec-6-2,Section 6.2>> is powered by CNNs. When you upload an image and receive a caption or object detection result, a convolutional neural network is performing the analysis behind the scenes.


[[sec-6-11]]
== From CNNs to Multimodal Models

.Figure 6.9: Multimodal Model Architecture
image::ch6/visual_6_9_multimodal_architecture.png[Dual-input flow diagram showing image and text streams converging in shared embedding space,width=85%]


=== The Rise of Transformers

In recent years, a neural network architecture called the *transformer* has reshaped the field of AI. Originally developed for natural language processing (NLP) (explored further in xref:chapter-7.adoc#ch-7[Chapter 7]: NLP Workloads on Azure), transformers process input data by converting it into numerical representations called *embeddings*. These embeddings capture the meaning and relationships within the data, positioning similar concepts close together in a mathematical space.

=== Multimodal Models

The success of transformers with text led researchers to apply similar principles to images, and eventually to build *multimodal models* that can process both images and text simultaneously. These models work by:

[arabic]
. Analyzing an image to extract visual features
. Converting accompanying text into embeddings
. Learning the relationships between the visual features and the text embeddings

This allows a single model to perform tasks that span both visual and linguistic understanding.

=== Capabilities of Multimodal Models

[width="100%",cols="50%,50%",options="header",]
|===
|Capability |Description
|*Image captioning* |Generating natural-language descriptions of images
|*Visual question answering* |Answering questions about the content of an image
|*Image classification* |Categorizing images based on combined visual and textual cues
|*Image tagging* |Assigning relevant descriptive keywords to images
|*Image search* |Retrieving images based on natural-language queries
|===

Microsoft's *Florence* foundation model is an example of a multimodal model trained on millions of image-caption pairs. It serves as the backbone for several capabilities within Azure AI Vision.

[.definition]
.Definition: Multimodal Model
****
An AI model capable of processing and relating multiple types of input data, such as images and text, within a single framework. Multimodal models can generate text from images, answer questions about visual content, and perform tasks that require understanding both modalities.
****


[.definition]
.Definition: Transformer
****
A neural network architecture that processes input data using a mechanism called self-attention, enabling it to capture long-range relationships. Originally developed for NLP, transformers now underpin many state-of-the-art vision and multimodal models.
****


[.reflection]
.Reflection
****
Multimodal models combine vision and language understanding. Think of a scenario in education, customer service, or content creation where a system that can both "`see`" and "`read`" would provide value that neither capability alone could deliver.
****



[[sec-6-12]]
== Responsible AI and Computer Vision

Computer vision raises specific ethical considerations that go beyond general responsible AI principles. The ability to analyze images of people, track objects in video, and extract text from documents carries risks that must be actively managed.

=== Fairness

Research has demonstrated that facial recognition systems can exhibit bias, particularly against individuals with darker skin tones and women. These biases typically originate from training datasets that underrepresent certain demographic groups. Mitigating bias requires:

* Building diverse and representative training datasets
* Using fairness-aware evaluation metrics during model development
* Conducting regular audits of model performance across demographic groups

Microsoft has responded to these concerns by restricting access to facial recognition features, restricting access to capabilities that inferred sensitive attributes such as emotional state and gender, and applying additional scrutiny to age estimation features.

=== Privacy and Security

Computer vision systems that process images of people raise significant privacy concerns:

* *Consent:* Individuals may not know their images are being captured or analyzed
* *Surveillance:* The technology can enable mass surveillance without appropriate oversight
* *Data breaches:* Stored facial data, if compromised, can lead to identity theft or unauthorized tracking

Organizations deploying computer vision should obtain informed consent, apply strong encryption to stored visual data, comply with data protection regulations (such as the GDPR), and provide clear opt-out mechanisms.

=== Transparency

Users and affected individuals should understand how computer vision systems work, what data they collect, and how decisions are made. Transparency practices include:

* Publishing clear documentation about the algorithms and data sources used
* Providing explanations of how the system reaches its conclusions (explainable AI)
* Communicating openly with stakeholders about the system's capabilities and limitations

[.definition]
.Definition: Responsible AI
****
A framework of principles — including fairness, reliability, safety, privacy, inclusiveness, transparency, and accountability — that guides the ethical development and deployment of AI systems. In computer vision, responsible AI is especially critical due to the technology's ability to process images of people and sensitive documents.
****


[TIP]
====
For the AI-900 exam, remember that responsible AI in computer vision is not just about building accurate models. It also involves ensuring fairness across demographic groups, protecting individual privacy, and being transparent about how the technology works and what data it collects.
====



== Chapter Summary

* *Computer vision* enables machines to interpret visual data and accounts for 15–20% of the AI-900 exam.
* Azure provides multiple computer vision services: *Azure AI Vision* (general-purpose), *Azure AI Services* (unified multi-capability endpoint), *Azure AI Custom Vision* (custom-trained models), *Azure AI Face* (facial detection and recognition), and *Azure AI Video Indexer* (video analysis).
* Core capabilities include *image captioning*, *tagging*, *object detection*, *facial detection*, *face recognition*, and *OCR*, each returning structured JSON results with confidence scores.
* Digital images are represented as grids of *pixels*. Grayscale images use one channel (0–255), while color images use three *RGB channels*.
* *Image filters* use kernels to transform images through convolution, enabling effects such as blurring, sharpening, and edge detection.
* *Image classification* assigns a label to an entire image; *object detection* identifies and locates specific objects with bounding boxes.
* *OCR* extracts text from images. Azure's Read API supports both real-time and batch processing. *Document Intelligence* extends OCR by understanding document structure.
* *Facial detection* finds faces; *face recognition* identifies individuals. Microsoft restricts access to recognition features under its responsible AI policies.
* *CNNs* are the primary deep learning architecture for vision tasks, using convolutional, pooling, and fully connected layers trained through backpropagation.
* *Multimodal models* combine image and text understanding using transformer architectures, enabling capabilities like visual question answering and image captioning.
* *Responsible AI* in computer vision requires attention to *fairness* (mitigating demographic bias), *privacy* (consent and data protection), and *transparency* (clear documentation and explainability).


== Key Terms

[width="100%",cols="50%,50%",options="header",]
|===
|Term |Definition
|*Computer Vision* |A field of AI focused on enabling computers to interpret and extract meaningful information from visual data such as images and video
|*Pixel* |The smallest unit of a digital image, represented by numerical values that encode brightness and color information
|*RGB (Red, Green, Blue)* |A color model that combines red, green, and blue light channels, each with intensity values from 0 to 255, to represent the full color spectrum
|*Resolution* |The total number of pixels in an image, typically expressed as width times height (e.g., 1920 x 1080)
|*Kernel (Filter Matrix)* |A small grid of numbers used in convolution operations to transform an image by modifying pixel values based on their neighbors
|*Convolution* |A mathematical operation that slides a kernel across an image, computing weighted sums of pixel neighborhoods to produce a transformed output
|*Feature Map* |The output produced by applying a convolutional filter to an image, highlighting specific patterns such as edges or textures
|*Image Classification* |The task of assigning a category label to an entire image based on its visual content
|*Object Detection* |The task of identifying and locating specific objects within an image, typically using bounding boxes with coordinate data
|*Bounding Box* |A rectangular outline defined by pixel coordinates that frames a detected object within an image
|*Confidence Score* |A numerical value (typically 0 to 1) representing how certain an AI model is about a particular prediction
|*OCR (Optical Character Recognition)* |Technology that extracts text from images, converting visual representations of characters into machine-readable digital text
|*Document Intelligence* |An Azure AI service that extends OCR by extracting structured information such as fields, tables, and key-value pairs from documents
|*Facial Detection* |The process of locating human faces within an image without identifying the individuals
|*Face Recognition* |The process of matching a detected face against a database of known identities to determine who the person is
|*Facial Landmarks* |Specific reference points on a face (e.g., eye corners, nose tip, mouth edges) identified by facial analysis models and returned as coordinate data
|*CNN (Convolutional Neural Network)* |A deep learning architecture designed for processing grid-structured data, using convolutional layers to extract spatial features from images
|*Pooling* |A CNN operation that reduces the spatial dimensions of feature maps, retaining the most important information while decreasing computational cost
|*Backpropagation* |The process of propagating prediction errors backward through a neural network to adjust weights and improve future predictions
|*Activation Function* |A mathematical function applied to the output of a neural network layer to introduce non-linearity; softmax is commonly used in the output layer for classification
|*Transformer* |A neural network architecture using self-attention mechanisms to capture relationships in data; foundational to modern NLP and multimodal models
|*Embedding* |A numerical vector representation of data (such as a word or image feature) positioned in a mathematical space where similar items are near each other
|*Multimodal Model* |An AI model that processes and relates multiple types of input (e.g., images and text) within a single framework
|*Supervised Learning* |A machine learning approach in which the model is trained on labeled data — examples where the correct answer is already known — enabling it to learn patterns and make predictions on new data (see xref:chapter-4.adoc#ch-4[Chapter 4] for full coverage)
|*Responsible AI* |A framework of principles — including fairness, reliability, safety, privacy, inclusiveness, transparency, and accountability — guiding ethical AI development and deployment
|===


== Review Questions

Test your understanding of the material covered in this chapter.

[arabic]
. *A company needs a single Azure endpoint to handle image analysis, text translation, and speech recognition in one application. Which Azure service should they use, and why?*
. *Explain the difference between a grayscale image and a color image in terms of how pixel data is stored. How many numerical values describe a single pixel in each case?*
. *A retail company wants to automatically monitor which products are present on store shelves and where they are positioned. Should they use image classification or object detection? Justify your answer.*
. *Describe the role of each major layer type in a CNN: convolutional layers, pooling layers, and fully connected layers. How do they work together to produce a prediction?*
. *A healthcare organization wants to digitize decades of handwritten patient records stored on paper. Which Azure capability would they use, and what information would the API response include beyond the extracted text?*
. *What is the distinction between facial detection and face recognition? Give a scenario where you would need detection but _not_ recognition.*
. *Explain how multimodal models differ from traditional CNNs. What advantage does combining visual and textual understanding provide?*
. *A facial recognition system performs well on lighter-skinned individuals but shows significantly lower accuracy for individuals with darker skin tones. What responsible AI principle does this violate, what is the likely cause, and how can it be addressed?*
. *Describe the purpose of a kernel (filter) in convolutional image processing. How does applying a kernel to an image produce a feature map, and why does the choice of kernel affect what visual patterns are detected?*
. *A wildlife conservation organization wants to build an AI system that can identify individual endangered animals from camera trap photos. The system needs to recognize specific species that are not included in any prebuilt model. Should they use Azure AI Vision or Azure AI Custom Vision? Explain your reasoning.*


== Additional Resources

* https://learn.microsoft.com/en-us/training/modules/analyze-images-computer-vision/[Microsoft Learn: Fundamentals of Azure AI Vision] (Free)
* https://learn.microsoft.com/en-us/training/modules/detect-analyze-faces/[Microsoft Learn: Detect and Analyze Faces with the Face Service] (Free)
* https://learn.microsoft.com/en-us/training/modules/read-text-computer-vision/[Microsoft Learn: Read Text in Images and Documents with the Azure AI Vision Service] (Free)
* https://learn.microsoft.com/en-us/training/modules/classify-images-custom-vision/[Microsoft Learn: Image Classification with Custom Vision] (Free)
* https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/[Azure AI Vision Documentation] (Official documentation)
* https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview-identity[Azure AI Face Documentation] (Official documentation)
* http://cs231n.stanford.edu/[Stanford CS231n: Convolutional Neural Networks for Visual Recognition] (Free lecture notes and videos)
* https://www.elementsofai.com/[Elements of AI] (Free course, University of Helsinki)
