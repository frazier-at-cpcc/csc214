
[[ch-5]]
= Azure Machine Learning


== Learning Objectives

By the end of this chapter, you will be able to:

[arabic]
. Describe the purpose and core capabilities of Azure Machine Learning as a cloud-based platform
. Explain how an Azure Machine Learning workspace organizes resources for model development
. Compare Automated Machine Learning (AutoML) and Azure Machine Learning Designer as model-building approaches
. Outline the end-to-end workflow for training, evaluating, and deploying a model using AutoML
. Describe how to construct a visual machine learning pipeline using Azure Machine Learning Designer
. Identify common evaluation metrics used to assess model performance in Azure Machine Learning
. Explain how Azure Machine Learning supports responsible AI practices throughout the model lifecycle


[[sec-5-1]]
== What Is Azure Machine Learning?

Azure Machine Learning is a cloud service from Microsoft designed to support the full lifecycle of machine learning projects. It provides tools for preparing data, training models, evaluating results, and deploying solutions — all within a managed cloud environment.

What makes Azure Machine Learning distinctive is that it serves a broad range of users. Data scientists who prefer writing code in Python or R can work in notebooks and scripts. Business analysts or citizen developers who prefer visual interfaces can use drag-and-drop tools. The platform is designed so that users with different skill levels and preferences can all participate in building machine learning solutions.

Azure Machine Learning is not limited to Microsoft's own algorithms or frameworks. It integrates with widely used open-source libraries such as *PyTorch*, *TensorFlow*, and *scikit-learn* (popular programming tools and libraries used by data scientists to build models), giving practitioners the freedom to use the tools they already know while benefiting from Azure's managed infrastructure.

[.definition]
.Definition: Azure Machine Learning
****
A cloud-based service on Microsoft Azure that provides end-to-end tools for building, training, evaluating, deploying, and managing machine learning models. It supports both code-first and visual development experiences.
****



[[sec-5-2]]
== The Azure Machine Learning Workspace

.Figure 5.3: Azure ML Workspace Architecture
image::ch5/visual_5_3_workspace_architecture.png[Hub-and-spoke diagram showing workspace connected to datastores; compute; experiments; models; endpoints; and pipelines,width=85%]


Before you can build models or run experiments, you need a centralized place to organize your work. In Azure Machine Learning, that place is called a *workspace*.

A workspace is the top-level resource for Azure Machine Learning. Think of it as a project headquarters where all related assets — datasets, compute resources, experiments, models, and endpoints — are stored and managed together. When you create a workspace within an Azure subscription, several supporting resources are provisioned as needed, including a storage account for data, a container registry for model images (created when first required), and an Application Insights instance for monitoring.

=== Key Components of a Workspace

The following table summarizes the main assets organized within an Azure Machine Learning workspace.

[width="100%",cols="50%,50%",options="header",]
|===
|Component |Purpose
|*Datastores* |Connections to storage locations (e.g., Azure Blob Storage, Azure Data Lake) where training data resides
|*Compute resources* |Virtual machines or clusters (also called compute clusters for large-scale training jobs) used to run training jobs, notebooks, and inference endpoints
|*Experiments* |Logical containers that group individual training runs so you can compare results
|*Models* |Trained model artifacts registered in the workspace for versioning and deployment
|*Endpoints* |Deployment targets (real-time or batch) where models are published for consumption by applications
|*Pipelines* |Multi-step workflows that automate data preparation, training, and evaluation tasks
|===

=== Azure Machine Learning Studio

Once a workspace exists, you interact with it primarily through *Azure Machine Learning Studio*, a browser-based portal. The studio provides a unified interface where you can import data, launch experiments, monitor training runs, review evaluation metrics, and deploy models — all without leaving your browser.

[.definition]
.Definition: Azure ML Studio
****
A browser-based portal for interacting with an Azure Machine Learning workspace, providing access to tools like AutoML and the Designer without requiring local software installation.
****


[.definition]
.Definition: Azure ML Workspace
****
The top-level Azure resource that serves as the central hub for all machine learning assets, including data, compute, models, and deployments. Every Azure Machine Learning project begins with creating or selecting a workspace.
****


[TIP]
====
For the AI-900 exam, you do not need to memorize how to create a workspace step by step. Focus on understanding what a workspace is, what it contains, and why it serves as the foundation for Azure Machine Learning projects.
====



[[sec-5-3]]
== Core Capabilities of Azure Machine Learning

.Figure 5.5: ML Lifecycle Overview
image::ch5/visual_5_5_ml_lifecycle.png[Circular lifecycle diagram showing data preparation through deployment and monitoring,width=85%]


Azure Machine Learning offers a broad set of capabilities that span the entire model lifecycle. Understanding these capabilities at a high level is essential for the AI-900 exam.

=== Data Preparation and Exploration

Before any model can be trained, data must be collected, cleaned, and understood. Azure Machine Learning allows you to register datasets within your workspace, explore their structure and distributions, and apply transformations to prepare them for modeling. Data can be uploaded from local files, connected from Azure storage services, or imported from external sources.

=== Model Training

Azure Machine Learning provides on-demand compute resources — from single virtual machines to multi-node clusters — that scale to meet the demands of your training jobs. You can submit training jobs through code (using the Python SDK or CLI) or through the visual tools described later in this chapter. The platform tracks every training run, logging metrics, parameters, and outputs so that you can compare experiments over time.

=== Model Evaluation

After training, you need to determine how well a model performs. Azure Machine Learning automatically records evaluation metrics for each run. For regression tasks, you might examine metrics like mean absolute error (MAE) or root mean squared error (RMSE). For classification tasks, metrics such as accuracy, precision, recall, and the F1 score are commonly reviewed. These metrics help you decide whether a model is ready for deployment or needs further tuning.

=== Model Registration and Management

Once a model meets your performance standards, you register it in the workspace. Registration creates a versioned record of the model artifact, making it easy to track which version is deployed, roll back to earlier versions, or compare performance across iterations.

For example, if a deployed churn prediction model is retrained on new data and achieves better accuracy, the new version can be registered alongside the original. If the update introduces unexpected behavior, the team can roll back to the prior registered version without rebuilding from scratch.

=== Model Deployment

.Figure 5.6: Real-Time vs. Batch Deployment
image::ch5/visual_5_6_deployment_types.png[Two-panel comparison of real-time endpoint and batch endpoint deployment types,width=85%]


Azure Machine Learning supports multiple deployment options.

*Table: Azure Machine Learning Deployment Types*

[width="100%",cols="50%,50%",options="header",]
|===
|Deployment Type |Description
|*Real-time endpoint* |The model is hosted as a web service that returns predictions immediately when called by an application
|*Batch endpoint* |The model processes large volumes of data in scheduled or on-demand batches, returning results asynchronously
|===

=== MLOps and Monitoring

*MLOps* (Machine Learning Operations) refers to practices that bring DevOps-style rigor to the machine learning lifecycle. Azure Machine Learning integrates with tools like *MLflow* for experiment tracking and model management. Once a model is deployed, you can monitor its performance over time, detect data drift (changes in input data that may degrade predictions), and redeploy updated models as needed.

For example, if a model trained on pre-pandemic purchasing behavior is still deployed two years later, the statistical distribution of input features may have shifted substantially — customers may now buy more online and less in-store. This shift can cause predictions to degrade over time, which is why monitoring for data drift is essential.

[.definition]
.Definition: MLOps
****
A set of practices that apply DevOps principles — such as version control, continuous integration, continuous deployment, and monitoring — to the machine learning lifecycle. The goal is to make model development and deployment reliable, reproducible, and scalable.
****


[.reflection]
.Reflection
****
Consider a scenario where a deployed model begins making less accurate predictions over several months. What might cause this, and how could monitoring and redeployment practices help address it?
****



[[sec-5-4]]
== Automated Machine Learning (AutoML)

.Figure 5.2: AutoML Process Diagram
image::ch5/visual_5_2_automl_process.png[Flow diagram showing AutoML pipeline from data input through algorithm selection to best model,width=85%]


One of the most powerful features of Azure Machine Learning is *Automated Machine Learning*, commonly called *AutoML*. AutoML is designed to simplify and accelerate the model development process by automating many of the decisions that traditionally require deep expertise.

=== What AutoML Automates

When building a machine learning model manually, a practitioner must make a series of decisions: which algorithm to try, how to preprocess the data, which hyperparameters to tune, and how to validate results. AutoML handles these decisions systematically. It tries multiple combinations of algorithms and settings, evaluates each one against a chosen metric, and identifies the best-performing model.

The tasks that AutoML can automate include:

* *Algorithm selection* — testing multiple algorithms (e.g., decision trees, gradient boosting, linear models) against your data
* *Feature engineering* — applying transformations to input features to improve model performance
* *Hyperparameter tuning* — adjusting the internal settings of each algorithm to find optimal configurations
* *Model evaluation* — scoring each candidate model using the metric you specify and ranking them by performance

[.definition]
.Definition: Hyperparameter
****
A configuration setting for a machine learning algorithm (such as learning rate or tree depth) that is set before training begins and directly influences how the model learns from data. AutoML automates hyperparameter tuning by testing many combinations to find the best settings.
****


=== Supported Task Types

.Figure 5.7: AutoML Supported Task Types
image::ch5/visual_5_7_automl_task_types.png[Three cards showing Classification; Regression; and Time-Series Forecasting task types,width=85%]


AutoML supports several categories of machine learning tasks.

*Table: AutoML Supported Task Types*

[width="100%",cols="34%,33%,33%",options="header",]
|===
|Task Type |Description |Example Use Case
|*Classification* |Predicting which category a data point belongs to |Determining whether a customer will churn (yes/no)
|*Regression* |Predicting a continuous numerical value |Estimating insurance costs based on patient demographics
|*Time-series forecasting* |Predicting future values based on historical trends |Forecasting monthly product demand
|===

[.definition]
.Definition: AutoML (Automated Machine Learning)
****
A capability within Azure Machine Learning that automates the process of selecting algorithms, engineering features, tuning hyperparameters, and evaluating models. It enables users to build high-quality models with less manual effort and specialized knowledge.
****


=== The AutoML Workflow

A typical AutoML workflow in Azure Machine Learning Studio follows these general steps:

[arabic]
. *Create a new Automated ML job* from the studio interface
. *Configure basic settings* such as the experiment name and description
. *Select the task type* (classification, regression, or time-series forecasting)
. *Provide training data* by uploading a dataset or selecting one already registered in the workspace
. *Choose the target column* — the variable the model should predict
. *Set the primary metric* that AutoML will use to rank candidate models (e.g., normalized root mean squared error for regression)
. *Configure limits* such as maximum number of trials, timeout duration, and early termination rules
. *Select compute resources* for the training job
. *Submit the job* and wait for AutoML to train and evaluate multiple models
. *Review results*, including the best model summary, evaluation metrics, and visualizations such as residual plots or predicted-versus-actual charts

=== Understanding AutoML Results

When an AutoML job completes, the studio presents a summary of the best-performing model along with its evaluation metrics. You can drill into individual metrics, view visualizations that show how predictions compare to actual values, and explore residual distributions to understand where the model performs well and where it struggles.

From this point, the best model can be deployed as a real-time endpoint, a batch endpoint, or a web service, making its predictions available to applications and users.

=== Deploying the Best Model

Once AutoML identifies the best-performing model, it can be deployed directly from Azure Machine Learning Studio. Deployment options include:

* *Real-time endpoint:* The model is hosted as a web service that returns predictions immediately when it receives input data. This is appropriate for applications that need instant responses, such as fraud detection systems.
* *Batch endpoint:* The model processes large volumes of data in scheduled batches rather than one request at a time. This suits scenarios like generating weekly sales forecasts.

The deployed model receives an endpoint URL and authentication key, similar to other Azure AI services described in xref:chapter-2.adoc#ch-2[Chapter 2]. Applications send data to the endpoint and receive predictions in return.

[TIP]
====
The AI-900 exam is unlikely to ask you to configure an AutoML job in detail. Instead, focus on what AutoML does (automates algorithm selection and tuning), what task types it supports, and why it is useful (reduces the expertise and effort required to build models).
====



[[sec-5-5]]
== Azure Machine Learning Designer

.Figure 5.1: Azure ML Designer Canvas
image::ch5/visual_5_1_azure_ml_designer.png[Mockup of the Designer canvas showing connected pipeline components from dataset to evaluation,width=85%]


While AutoML automates much of the model-building process, *Azure Machine Learning Designer* takes a different approach: it gives you a visual, drag-and-drop canvas for building machine learning pipelines step by step.

=== What Is the Designer?

The Designer is a graphical tool within Azure Machine Learning Studio that lets you construct machine learning workflows by dragging components onto a canvas and connecting them visually. Each component represents a specific operation — loading a dataset, splitting data, training a model, scoring predictions, or evaluating results. By connecting these components in sequence, you create a *pipeline* that defines your entire workflow from raw data to evaluated model.

The Designer is especially valuable for users who prefer a visual approach to understanding and building machine learning processes. It requires no coding, though it offers the same rigor as code-based approaches in terms of the operations it performs.

[.definition]
.Definition: Azure Machine Learning Designer
****
A drag-and-drop visual tool within Azure Machine Learning Studio that enables users to build, test, and deploy machine learning pipelines without writing code. Users connect pre-built components on a canvas to define data processing and model training workflows.
****


=== Key Components in the Designer

The Designer provides a library of pre-built components that you can search and drag onto the canvas. Here are some of the most commonly used categories.

*Table: Designer Component Categories*

[width="100%",cols="34%,33%,33%",options="header",]
|===
|Component Category |Examples |Purpose
|*Datasets* |Sample datasets (e.g., automobile prices, census income data) or custom uploads |Provide training and testing data
|*Data Transformation* |Split Data, Select Columns, Clean Missing Data, Normalize Data |Prepare and shape data before training
|*Machine Learning Algorithms* |Linear Regression, Logistic Regression, Decision Forest, Neural Network |Define the learning algorithm for the model
|*Model Training* |Train Model |Execute the learning process using the selected algorithm and data
|*Model Scoring* |Score Model |Generate predictions on test data using the trained model
|*Model Evaluation* |Evaluate Model |Calculate performance metrics for the scored predictions
|===

=== Building a Pipeline in the Designer

A typical Designer workflow follows this pattern:

[arabic]
. *Start a new pipeline* from the Designer section in Azure Machine Learning Studio
. *Add a dataset* by dragging it onto the canvas from the data library
. *Add a Split Data component* and connect it to the dataset. Configure the split ratio (e.g., 70% for training, 30% for testing) to create separate training and testing sets
. *Add a machine learning algorithm* (e.g., Linear Regression) and drag it onto the canvas
. *Add a Train Model component* and connect it to both the algorithm and the training output of the Split Data component. Specify the target column — the variable you want the model to predict
. *Add a Score Model component* and connect it to the trained model and the testing output of the Split Data component
. *Add an Evaluate Model component* and connect it to the Score Model output to generate performance metrics
. *Configure compute resources* by creating or selecting a compute cluster
. *Submit the pipeline* for execution
. *Review evaluation results* once the pipeline completes, examining metrics that indicate how well the model performs

=== Interpreting Designer Results

After a pipeline runs successfully, you can inspect the output of the Evaluate Model component to see performance metrics. For a regression pipeline, you would see metrics like MAE, RMSE, and the coefficient of determination (R-squared). For a classification pipeline, you might see accuracy, precision, recall, and the area under the ROC curve (AUC).

These metrics help you determine whether the model is performing well enough for your use case or whether you need to adjust the pipeline — perhaps by trying a different algorithm, adding data transformation steps, or changing the train-test split ratio.

[.reflection]
.Reflection
****
Imagine you are building a machine learning solution but have limited coding experience. How might the Designer's visual approach change the way you think about the model-building process compared to writing code? What advantages and limitations can you identify?
****



[[sec-5-6]]
== AutoML vs. Designer: Choosing the Right Tool

.Figure 5.4: AutoML vs. Designer Comparison
image::ch5/visual_5_4_automl_vs_designer.png[Two-column comparison showing AutoML automated approach vs. Designer visual step-by-step approach,width=85%]


Both AutoML and the Designer are available within Azure Machine Learning Studio, but they serve different purposes and suit different workflows. The table below summarizes the key differences.

*Table: AutoML vs. Designer Comparison*

[width="100%",cols="34%,33%,33%",options="header",]
|===
|Feature |AutoML |Designer
|*Approach* |Automated: the system selects and tunes models for you |Manual: you build the pipeline step by step using visual components
|*User control* |Less granular; you define the task, data, and constraints, then AutoML decides the rest |More granular; you choose each algorithm, transformation, and connection
|*Coding required* |No (studio interface) or optional (Python SDK) |No
|*Best for* |Quickly finding a strong baseline model; users who want results with minimal configuration |Users who want to understand and control every step of the pipeline; educational exploration
|*Task types* |Classification, regression, time-series forecasting |Any task supported by the available components (broader flexibility in pipeline design)
|*Speed to first result* |Faster (automated experimentation) |Slower (manual assembly) but more transparent
|===

In practice, many teams use both tools. AutoML can quickly establish a performance baseline, and the Designer can be used to build custom pipelines that incorporate specific preprocessing steps or algorithms not covered by AutoML.

[NOTE]
====
In Azure Machine Learning, a pipeline is a sequence of connected steps (components) that define a reproducible workflow for data processing, model training, scoring, and evaluation. Pipelines can be built visually in the Designer or defined in code.
====


[TIP]
====
For the AI-900 exam, be prepared to distinguish between AutoML and the Designer. Remember that AutoML automates the selection process, while the Designer gives you visual control over each step. Both are no-code options available in Azure Machine Learning Studio.
====



[[sec-5-7]]
== Evaluation Metrics in Azure Machine Learning

Regardless of whether you use AutoML or the Designer, understanding evaluation metrics is critical for assessing model quality. Azure Machine Learning surfaces these metrics automatically after training runs. The specific metrics displayed depend on the type of task.

=== Regression Metrics

*Table: Common Regression Evaluation Metrics in Azure Machine Learning*

[width="100%",cols="50%,50%",options="header",]
|===
|Metric |What It Measures
|*Mean Absolute Error (MAE)* |The average of the absolute differences between predicted and actual values. Lower is better.
|*Root Mean Squared Error (RMSE)* |The square root of the average of squared differences between predicted and actual values. Penalizes large errors more heavily than MAE. Lower is better.
|*Normalized Root Mean Squared Error (NRMSE)* |RMSE scaled relative to the range of the target variable, making it easier to compare across different datasets. Lower is better.
|*Coefficient of Determination (R-squared)* |The proportion of variance in the target variable that is explained by the model. Ranges from 0 to 1, where values closer to 1 indicate a better fit.
|===

=== Classification Metrics

*Table: Common Classification Evaluation Metrics in Azure Machine Learning*

[width="100%",cols="50%,50%",options="header",]
|===
|Metric |What It Measures
|*Accuracy* |The proportion of all predictions that were correct. Can be misleading with imbalanced datasets.
|*Precision* |Of all instances the model predicted as positive, the proportion that were actually positive.
|*Recall* |Of all actual positive instances, the proportion that the model correctly identified.
|*F1 Score* |The harmonic mean of precision and recall, providing a single balanced measure when both matter.
|*AUC (Area Under the ROC Curve)* |A measure of the model's ability to distinguish between classes across all threshold settings. Ranges from 0 to 1, where 0.5 indicates performance no better than random chance and values closer to 1.0 indicate stronger ability to distinguish between classes.
|===

[TIP]
====
A model trained to detect fraudulent credit card transactions predicts fraud in only 2% of all transactions. Accuracy is 98%. Is this a reliable indicator of model quality? Which metric would give you better insight, and why? (Hint: Consider what happens when the dataset is heavily imbalanced — 98% of transactions are legitimate.)
====


[NOTE]
====
*Exam Focus:* For the AI-900 exam, focus on understanding what each metric measures and when it is most useful. Accuracy, precision, recall, and AUC are the most commonly tested classification metrics. For regression, RMSE and R-squared appear most frequently.
====


[.definition]
.Definition: Evaluation Metric
****
A quantitative measure used to assess how well a machine learning model performs on a given task. Different metrics are appropriate for different task types (regression vs. classification) and different priorities (e.g., minimizing false positives vs. false negatives).
****



[[sec-5-8]]
== Responsible AI in Azure Machine Learning

.Figure 5.8: Responsible AI Dashboard
image::ch5/visual_5_8_responsible_ai_dashboard.png[Dashboard mockup showing model explainability; fairness assessment; error analysis; and data transparency panels,width=85%]


Azure Machine Learning includes built-in tools that support responsible AI practices throughout the model lifecycle. As discussed in xref:chapter-3.adoc#ch-3[Chapter 3] of this course (see xref:chapter-3.adoc#sec-3-11[Section 3.11]: Principles of Responsible AI), responsible AI is guided by principles such as fairness, reliability, safety, privacy, inclusiveness, transparency, and accountability.

Within Azure Machine Learning, responsible AI capabilities include:

* *Model explainability:* Tools that help you understand which features most influence a model's predictions, making the decision-making process more transparent
* *Fairness assessment:* Analysis that detects whether a model produces different outcomes for different demographic groups, helping identify and mitigate bias
* *Error analysis:* Dashboards that help you understand where and why a model makes errors, so you can improve performance for underserved groups or edge cases
* *Data transparency:* Clear tracking of which datasets were used for training and evaluation, supporting auditability and reproducibility

These tools are integrated into the Azure Machine Learning Studio, so practitioners can review responsible AI metrics alongside standard performance metrics when deciding whether a model is ready for deployment.

[.reflection]
.Reflection
****
Suppose you build an insurance cost prediction model that performs well on average but consistently overestimates costs for a specific demographic group. How would fairness assessment tools help you identify and address this issue before deployment?
****



== Chapter Summary

* *Azure Machine Learning* is a cloud-based platform that supports the full lifecycle of machine learning projects, from data preparation through model deployment and monitoring.
* A *workspace* is the foundational resource in Azure Machine Learning, organizing datasets, compute resources, experiments, models, and endpoints in one place.
* *Azure Machine Learning Studio* is the browser-based portal used to manage workspace assets and interact with tools like AutoML and the Designer.
* *Automated Machine Learning (AutoML)* automates algorithm selection, feature engineering, hyperparameter tuning, and model evaluation, enabling users to build high-quality models with minimal manual effort.
* AutoML supports *classification, regression, and time-series forecasting* task types.
* *Azure Machine Learning Designer* provides a drag-and-drop visual interface for building machine learning pipelines without code, giving users step-by-step control over data processing, training, scoring, and evaluation.
* Both AutoML and the Designer are *no-code tools* available within Azure Machine Learning Studio, but they differ in the level of automation and user control.
* *Evaluation metrics* such as MAE, RMSE, accuracy, precision, recall, and F1 score help practitioners assess model quality and make informed deployment decisions.
* Azure Machine Learning integrates *responsible AI* tools for model explainability, fairness assessment, and error analysis, supporting ethical and transparent model development.
* *MLOps* practices, including experiment tracking with MLflow, monitoring, and redeployment, ensure that deployed models remain reliable over time.


== Key Terms

[width="100%",cols="50%,50%",options="header",]
|===
|Term |Definition
|*Azure Machine Learning* |A cloud-based service on Microsoft Azure for building, training, evaluating, deploying, and managing machine learning models across the full project lifecycle
|*Workspace* |The top-level Azure resource that organizes all machine learning assets — data, compute, experiments, models, and endpoints — in a single manageable unit
|*Azure Machine Learning Studio* |A browser-based portal for interacting with an Azure Machine Learning workspace, providing access to tools like AutoML and the Designer
|*AutoML (Automated Machine Learning)* |A feature that automates algorithm selection, feature engineering, hyperparameter tuning, and model evaluation to produce optimized models with minimal manual configuration
|*Azure Machine Learning Designer* |A visual, drag-and-drop tool for constructing machine learning pipelines by connecting pre-built components on a canvas without writing code
|*Pipeline* |A defined sequence of connected steps — such as data ingestion, transformation, training, scoring, and evaluation — that can be executed as a reproducible workflow
|*Compute resources* |Virtual machines or clusters that perform data processing, model training, and inference tasks in Azure Machine Learning. Large-scale training jobs typically use compute clusters.
|*Endpoint* |A deployment target where a trained model is published to serve predictions, either in real time or through batch processing
|*MLOps* |Practices that apply DevOps principles (version control, CI/CD, monitoring) to the machine learning lifecycle to ensure reliability, reproducibility, and scalability
|*Evaluation metric* |A quantitative measure (e.g., MAE, RMSE, accuracy, F1 score) used to assess how well a trained model performs on a given task
|*Hyperparameter* |A configuration setting for a machine learning algorithm that is set before training begins and influences how the model learns from data
|*Data drift* |A change in the statistical properties of input data over time that can degrade the performance of a deployed model
|*Model explainability* |Tools and techniques that reveal which features or inputs most influence a model's predictions, supporting transparency and trust
|===


== Review Questions

Test your understanding of the material covered in this chapter.

[arabic]
. *In your own words, describe what Azure Machine Learning is and identify four distinct capabilities it provides across the machine learning lifecycle.*
. *What is the role of a workspace in Azure Machine Learning, and what types of assets does it organize?*
. *Explain the difference between AutoML and Azure Machine Learning Designer. In what scenario might you choose one over the other?*
. *A data scientist submits an AutoML job for a regression task and selects "`normalized root mean squared error`" as the primary metric. What does this metric measure, and why might it be preferred over standard RMSE?*
. *Describe the sequence of components you would connect in Azure Machine Learning Designer to build a basic classification pipeline, from data input through evaluation.*
. *An AutoML job tests three different algorithms and reports that one achieves an NRMSE of 0.081 while the others score 0.12 and 0.15. Which model performed best, and how do you know?*
. *What is data drift, and how does it relate to the need for MLOps practices like monitoring and model redeployment?*
. *How does Azure Machine Learning support responsible AI? Name at least two specific capabilities and explain how they help ensure ethical model development.*
. *A team with no programming experience needs to build a machine learning model to predict customer churn. They want to understand each step of the process rather than having it fully automated. Which Azure Machine Learning tool would you recommend, and why?*


== Additional Resources

* https://learn.microsoft.com/en-us/azure/machine-learning/[Microsoft Learn: Azure Machine Learning Documentation] (Free)
* https://learn.microsoft.com/en-us/training/modules/intro-to-azure-ml/[Microsoft Learn: Introduction to Azure Machine Learning] (Free learning module)
* https://learn.microsoft.com/en-us/training/modules/use-automated-machine-learning/[Microsoft Learn: Explore Automated Machine Learning in Azure ML] (Free learning module)
* https://learn.microsoft.com/en-us/training/modules/create-regression-model-azure-machine-learning-designer/[Microsoft Learn: Create a Regression Model with Azure ML Designer] (Free learning module)
* https://ml.azure.com/[Azure Machine Learning Studio] (Free to explore with an Azure account)
* https://www.microsoft.com/en-us/ai/responsible-ai[Responsible AI Resources from Microsoft] (Free)
